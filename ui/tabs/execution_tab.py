import sys
import os

print(f"DEBUG: __file__ = {os.path.abspath(__file__)}")

# ã“ã“ã‚’ä¿®æ­£: ç›¸å¯¾ãƒ‘ã‚¹ã‚’ '../../' ã«å¤‰æ›´
calculated_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))

print(f"DEBUG: Calculated path to add = {calculated_path}")

sys.path.append(calculated_path)

# ãƒ‡ãƒãƒƒã‚°ç”¨: sys.path ã®å†…å®¹ã‚’å‡ºåŠ›
print("DEBUG: Current sys.path:")
for p in sys.path:
    print(f"- {p}")
import streamlit as st
import datetime
from config.models import get_model_config
from core import GeminiEvaluator, GitManager # GitManagerã¨GeminiEvaluatorã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ui.components import render_response_box, render_evaluation_box # å¿…è¦ã«å¿œã˜ã¦ä»–ã®UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– (execution_tab å°‚ç”¨ã®ã‚‚ã®ã¯ render_execution_tab ã®ä¸­ã§å‘¼ã³å‡ºã™)
def _initialize_session_state():
    """execution_tabã§ä½¿ã‚ã‚Œã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ã‚­ãƒ¼ã‚’åˆæœŸåŒ–"""
    defaults = {
        'execution_memo': "",
        'execution_mode': "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›",
        'prompt_template': "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{user_input}",
        'user_input_data': "",
        'single_prompt': "",
        'evaluation_criteria': """1. æ­£ç¢ºæ€§ï¼ˆ30ç‚¹ï¼‰
2. ç¶²ç¾…æ€§ï¼ˆ25ç‚¹ï¼‰
3. åˆ†ã‹ã‚Šã‚„ã™ã•ï¼ˆ25ç‚¹ï¼‰
4. è«–ç†æ€§ï¼ˆ20ç‚¹ï¼‰""",
        'latest_execution_result': None # å®Ÿè¡Œï¼‹è©•ä¾¡ã®æœ€çµ‚çµæœ
    }
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

def render_execution_tab():
    """æ”¹å–„ã•ã‚ŒãŸå®Ÿè¡Œã‚¿ãƒ–ï¼ˆä¸€æ¬¡å®Ÿè¡Œçµæœè¡¨ç¤ºå¾Œã€è‡ªå‹•è©•ä¾¡ï¼‰"""
    _initialize_session_state() # å®Ÿè¡Œã‚¿ãƒ–å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–

    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¯ã€ãã‚Œã‚‰ã‚’ä½¿ç”¨ã™ã‚‹é–¢æ•°ã®ã‚¹ã‚³ãƒ¼ãƒ—å†…ã§å®šç¾©
    # ã‚‚ã— _execute_prompt_and_evaluation_sequentially ãŒã“ã®é–¢æ•°ã®å¤–ã«ã‚ã‚‹ãªã‚‰ã€å¼•æ•°ã§æ¸¡ã™
    # ã“ã“ã§ã¯åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å†…ãªã®ã§ã€ render_execution_tab ã‚¹ã‚³ãƒ¼ãƒ—ã§å®šç¾©ã—ã€å†…éƒ¨é–¢æ•°ã«æ¸¡ã™ã‹ã€
    # _execute_prompt_and_evaluation_sequentially ãŒç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
    # ç°¡å˜ã®ãŸã‚ã€_execute_prompt_and_evaluation_sequentially ã®ä¸­ã§ st.empty() ã‚’å‘¼ã³å‡ºã™å½¢ã«ã™ã‚‹ã€‚

    exec_col1, exec_col2 = st.columns([3, 1])
    with exec_col1:
        st.markdown("### æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè¡Œã—è‡ªå‹•è©•ä¾¡")
    with exec_col2:
        current_branch = GitManager.get_current_branch()
        st.markdown(f"**ãƒ–ãƒ©ãƒ³ãƒ:** `{current_branch}`")

    with st.form("execution_form", clear_on_submit=False):
        memo_col1, memo_col2 = st.columns([4, 1])
        with memo_col1:
            execution_memo = st.text_input(
                "ğŸ“ å®Ÿè¡Œãƒ¡ãƒ¢", value=st.session_state.execution_memo,
                placeholder="å¤‰æ›´å†…å®¹ã‚„å®Ÿé¨“ç›®çš„...", key="memo_input_form"
            )
        with memo_col2:
            execution_mode_display = st.radio(
                "ãƒ¢ãƒ¼ãƒ‰", ["ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ", "å˜ä¸€"],
                index=0 if st.session_state.execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›" else 1,
                horizontal=True, key="mode_radio_form"
            )
        execution_mode_full = "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›" if execution_mode_display == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ" else "å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"

        prompt_template, user_input_data, single_prompt = _render_prompt_section_form(execution_mode_full)
        evaluation_criteria = _render_evaluation_section_form()
        submitted = st.form_submit_button("ğŸš€ å®Ÿè¡Œ & è‡ªå‹•è©•ä¾¡", type="primary", use_container_width=True)

    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ãƒ•ã‚©ãƒ¼ãƒ ã®å¤–ã€ã‹ã¤streamlitã‚³ãƒãƒ³ãƒ‰ãŒåˆã‚ã¦å®Ÿè¡Œã•ã‚Œã‚‹å‰ã«å®šç¾©
    # (st.set_page_configã‚ˆã‚Šå¾Œã€å®Ÿéš›ã®è¡¨ç¤ºè¦ç´ ã‚ˆã‚Šå‰)
    # â†’ _execute_prompt_and_evaluation_sequentially ã®ä¸­ã§å‘¼ã³å‡ºã™ã‚ˆã†ã«å¤‰æ›´

    if submitted:
        st.session_state.execution_memo = execution_memo
        st.session_state.execution_mode = execution_mode_full
        st.session_state.prompt_template = prompt_template
        st.session_state.user_input_data = user_input_data
        st.session_state.single_prompt = single_prompt
        st.session_state.evaluation_criteria = evaluation_criteria

        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ã“ã“ã§ä½œæˆã—ã€é–¢æ•°ã«æ¸¡ã™
        placeholder_intermediate_resp = st.empty()
        placeholder_intermediate_metrics = st.empty()
        placeholder_final_eval_info = st.empty() # è©•ä¾¡å¤±æ•—æ™‚ãªã©ã®æƒ…å ±è¡¨ç¤ºç”¨

        _execute_prompt_and_evaluation_sequentially(
            execution_memo, execution_mode_full,
            prompt_template, user_input_data, single_prompt, evaluation_criteria,
            placeholder_intermediate_resp, placeholder_intermediate_metrics, placeholder_final_eval_info
        )

    if st.session_state.latest_execution_result:
        st.markdown("---")
        st.subheader("âœ… å®Ÿè¡Œãƒ»è©•ä¾¡å®Œäº†çµæœ")
        _display_latest_results()

def _render_prompt_section_form(execution_mode):
    st.markdown("### ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    if execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›":
        template_col1, template_col2 = st.columns(2)
        with template_col1:
            st.markdown("**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**")
            prompt_template = st.text_area(
                "", value=st.session_state.prompt_template, height=200,
                placeholder="{user_input}ã§ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§", key="template_area_form", label_visibility="collapsed"
            )
        with template_col2:
            st.markdown("**ãƒ‡ãƒ¼ã‚¿**")
            user_input_data = st.text_area(
                "", value=st.session_state.user_input_data, height=200,
                placeholder="å‡¦ç†ã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›...", key="data_area_form", label_visibility="collapsed"
            )
        if prompt_template and user_input_data and "{user_input}" in prompt_template:
            if st.checkbox("ğŸ” æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºèª", key="preview_form"):
                final_prompt_preview = prompt_template.replace("{user_input}", user_input_data)
                st.code(final_prompt_preview[:500] + "..." if len(final_prompt_preview) > 500 else final_prompt_preview)
        elif prompt_template and "{user_input}" not in prompt_template:
            st.warning("âš ï¸ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«{user_input}ã‚’å«ã‚ã¦ãã ã•ã„")
        return prompt_template, user_input_data, st.session_state.single_prompt # single_promptã‚‚è¿”ã™
    else:  # å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        st.markdown("**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**")
        single_prompt = st.text_area(
            "", value=st.session_state.single_prompt, height=200,
            placeholder="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...", key="single_area_form", label_visibility="collapsed"
        )
        return st.session_state.prompt_template, st.session_state.user_input_data, single_prompt # prompt_templateç­‰ã‚‚è¿”ã™

def _render_evaluation_section_form():
    st.markdown("### ğŸ“‹ è©•ä¾¡åŸºæº–")
    evaluation_criteria = st.text_area(
        "", value=st.session_state.evaluation_criteria, height=120,
        key="criteria_area_form", label_visibility="collapsed"
    )
    return evaluation_criteria

def _execute_prompt_and_evaluation_sequentially(
    execution_memo, execution_mode, prompt_template_val, user_input_data_val, single_prompt_val, evaluation_criteria_val,
    placeholder_intermediate_resp, placeholder_intermediate_metrics, placeholder_final_eval_info):

    placeholder_intermediate_resp.empty()
    placeholder_intermediate_metrics.empty()
    placeholder_final_eval_info.empty()
    st.session_state.latest_execution_result = None

    validation_errors = _validate_inputs_direct(execution_memo, execution_mode, evaluation_criteria_val, prompt_template_val, user_input_data_val, single_prompt_val)
    if validation_errors:
        for error in validation_errors:
            st.error(error)
        return

    if not st.session_state.api_key:
        st.error("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    model_config = get_model_config(st.session_state.selected_model)
    evaluator = GeminiEvaluator(st.session_state.api_key, model_config)

    final_prompt = ""
    current_prompt_template = None
    current_user_input = None
    if execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›":
        final_prompt = prompt_template_val.replace("{user_input}", user_input_data_val)
        current_prompt_template = prompt_template_val
        current_user_input = user_input_data_val
    else:
        final_prompt = single_prompt_val

    initial_execution_result = None
    with st.spinner(f"ğŸ”„ {model_config['name']}ã§ä¸€æ¬¡å®Ÿè¡Œä¸­..."):
        initial_execution_result = evaluator.execute_prompt(final_prompt)

    if not initial_execution_result or not initial_execution_result['success']:
        st.error(f"âŒ ä¸€æ¬¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {initial_execution_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        return

    with placeholder_intermediate_resp.container():
        st.markdown("---")
        st.subheader("ğŸ“ ä¸€æ¬¡å®Ÿè¡Œçµæœ (è©•ä¾¡å‰)")
        exec_res_disp = initial_execution_result
        render_response_box(exec_res_disp['response_text'], f"ğŸ¤– LLMã®å›ç­” ({exec_res_disp.get('model_name', '')})")

    with placeholder_intermediate_metrics.container():
        st.markdown("##### ğŸ“Š ä¸€æ¬¡å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        cols_metrics = st.columns(3)
        cols_metrics[0].metric("å®Ÿè¡Œå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_execution_result.get('input_tokens', 0):,}")
        cols_metrics[1].metric("å®Ÿè¡Œå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_execution_result.get('output_tokens', 0):,}")
        cols_metrics[2].metric("å®Ÿè¡Œã‚³ã‚¹ãƒˆ(USD)", f"${initial_execution_result.get('cost_usd', 0.0):.6f}")
        st.info("è©•ä¾¡å‡¦ç†ã‚’è‡ªå‹•çš„ã«é–‹å§‹ã—ã¾ã™...")

    evaluation_result = None
    with st.spinner("ğŸ“Š è©•ä¾¡å‡¦ç†ã‚’å®Ÿè¡Œä¸­..."):
        evaluation_result = evaluator.evaluate_response(
            original_prompt=final_prompt,
            llm_response_text=initial_execution_result['response_text'],
            evaluation_criteria=evaluation_criteria_val
        )

    if not evaluation_result or not evaluation_result['success']:
        with placeholder_final_eval_info.container():
            st.error(f"âŒ è©•ä¾¡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {evaluation_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            st.warning("ä¸€æ¬¡å®Ÿè¡Œã®çµæœã¯ä¸Šè¨˜ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ãŒã€è©•ä¾¡ã¯å¤±æ•—ã—ã¾ã—ãŸã€‚è¨˜éŒ²ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚")
        return

    placeholder_intermediate_resp.empty()
    placeholder_intermediate_metrics.empty()
    placeholder_final_eval_info.empty()

    execution_data_to_save = {
        'timestamp': datetime.datetime.now(),
        'execution_mode': execution_mode,
        'prompt_template': current_prompt_template,
        'user_input': current_user_input,
        'final_prompt': final_prompt,
        'criteria': evaluation_criteria_val,
        'response': initial_execution_result['response_text'],
        'evaluation': evaluation_result['response_text'],
        'execution_tokens': initial_execution_result['total_tokens'],
        'evaluation_tokens': evaluation_result['total_tokens'],
        'execution_cost': initial_execution_result['cost_usd'],
        'evaluation_cost': evaluation_result['cost_usd'],
        'total_cost': initial_execution_result['cost_usd'] + evaluation_result['cost_usd'],
        'model_name': initial_execution_result['model_name'],
        'model_id': initial_execution_result['model_id']
    }
    execution_record = GitManager.create_commit(execution_data_to_save, execution_memo)
    GitManager.add_commit_to_history(execution_record)

    st.session_state.latest_execution_result = {
        'execution_result': initial_execution_result,
        'evaluation_result': evaluation_result,
        'execution_record': execution_record
    }
    st.success(f"âœ… å®Ÿè¡Œã¨è©•ä¾¡ãŒå®Œäº†ã—ã€è¨˜éŒ²ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ | ID: `{execution_record['commit_hash']}`")
    st.rerun()

def _validate_inputs_direct(execution_memo, execution_mode, evaluation_criteria, prompt_template, user_input_data, single_prompt):
    errors = []
    if not execution_memo.strip():
        errors.append("âŒ å®Ÿè¡Œãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›":
        if not prompt_template.strip():
            errors.append("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        elif "{user_input}" not in prompt_template:
            errors.append("âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«{user_input}ã‚’å«ã‚ã¦ãã ã•ã„")
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã®æ‰±ã„ (ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ã‹è¨±å®¹ã™ã‚‹ã‹)
        # if not user_input_data.strip():
        #     errors.append("âŒ å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        if not single_prompt.strip():
            errors.append("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not evaluation_criteria.strip():
        errors.append("âŒ è©•ä¾¡åŸºæº–ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    return errors

def _display_latest_results():
    if not st.session_state.latest_execution_result:
        return

    result_data = st.session_state.latest_execution_result
    initial_exec_res = result_data['execution_result'] # 'response_text', 'cost_usd' etc.
    eval_res = result_data['evaluation_result']       # 'response_text', 'cost_usd' etc.
    # exec_record = result_data['execution_record'] # 'final_prompt'ãªã©ã‚’å«ã‚€ä¿å­˜ã•ã‚ŒãŸè¨˜éŒ²

    result_col1, result_col2 = st.columns([2, 1])
    with result_col1:
        render_response_box(initial_exec_res['response_text'], "ğŸ¤– LLMã®å›ç­”")
        render_evaluation_box(eval_res['response_text'], "â­ è©•ä¾¡çµæœ")
    with result_col2:
        st.markdown("### ğŸ“Š å®Ÿè¡Œãƒ»è©•ä¾¡æƒ…å ±")
        st.metric("ãƒ¢ãƒ‡ãƒ«å", initial_exec_res.get('model_name', 'N/A'))
        st.markdown("---")
        st.markdown("**å®Ÿè¡Œçµæœ**")
        cols_exec_final = st.columns(2) # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´ä¾‹
        with cols_exec_final[0]:
            st.metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('input_tokens', 0):,}")
            st.metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('total_tokens', 0):,}")
        with cols_exec_final[1]:
            st.metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('output_tokens', 0):,}")
            st.metric("ã‚³ã‚¹ãƒˆ(USD)", f"${initial_exec_res.get('cost_usd', 0.0):.6f}")
        st.markdown("---")
        st.markdown("**è©•ä¾¡å‡¦ç†**")
        cols_eval_final = st.columns(2) # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´ä¾‹
        with cols_eval_final[0]:
            st.metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{eval_res.get('input_tokens', 0):,}") # è©•ä¾¡å‡¦ç†ã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³
            st.metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³", f"{eval_res.get('total_tokens', 0):,}")
        with cols_eval_final[1]:
            st.metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{eval_res.get('output_tokens', 0):,}") # è©•ä¾¡å‡¦ç†ã®å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³
            st.metric("ã‚³ã‚¹ãƒˆ(USD)", f"${eval_res.get('cost_usd', 0.0):.6f}")
        st.markdown("---")
        total_cost_combined = initial_exec_res.get('cost_usd', 0.0) + eval_res.get('cost_usd', 0.0)
        st.metric("åˆè¨ˆã‚³ã‚¹ãƒˆ(USD)", f"${total_cost_combined:.6f}")

        # è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç­‰ã‚’è¡¨ç¤ºã™ã‚‹å ´åˆã¯ exec_record ã‚’ä½¿ç”¨
        # if 'execution_record' in result_data and result_data['execution_record']:
        #     final_prompt_disp = result_data['execution_record'].get('final_prompt', '')
        #     with st.expander("ğŸ“ å®Ÿè¡Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºèª"):
        #         st.code(final_prompt_disp, language=None)