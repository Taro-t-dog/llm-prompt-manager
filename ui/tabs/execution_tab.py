# ============================================
# ui/tabs/execution_tab.py (OpenAIå¯¾å¿œ)
# ============================================
import sys
import os
import streamlit as st
import datetime
import json
import time
from typing import Dict, List, Any, Optional, Tuple, Union # Added Union

# ãƒ‘ã‚¹è§£æ±º (ui/tabs/execution_tab.py ã‹ã‚‰è¦‹ãŸç›¸å¯¾ãƒ‘ã‚¹)
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root_from_tab = os.path.abspath(os.path.join(current_dir, "..", ".."))
if project_root_from_tab not in sys.path:
    sys.path.insert(0, project_root_from_tab)

from config import get_model_config # Direct import for model config
from core import GitManager, WorkflowEngine, WorkflowManager
from core.evaluator import GeminiEvaluator # For type hinting
from core.openai_evaluator import OpenAIEvaluator # For type hinting
from core.workflow_engine import StepResult, ExecutionStatus, WorkflowExecutionResult, WorkflowErrorHandler, VariableProcessor

from ui.components import (
    render_response_box, render_evaluation_box, render_workflow_result_tabs,
    render_error_details, render_workflow_step_card, render_workflow_live_step,
    render_workflow_execution_summary
)
from ui.styles import format_detailed_cost_display, format_tokens_display


def _initialize_session_state_exec_tab(): # Renamed to avoid conflict
    defaults = {
        'execution_memo': "",
        'execution_mode': "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›",
        'prompt_template': "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{user_input}",
        'user_input_data': "",
        'single_prompt': "",
        'evaluation_criteria': """1. æ­£ç¢ºæ€§ï¼ˆ30ç‚¹ï¼‰
2. ç¶²ç¾…æ€§ï¼ˆ25ç‚¹ï¼‰
3. åˆ†ã‹ã‚Šã‚„ã™ã•ï¼ˆ25ç‚¹ï¼‰
4. è«–ç†æ€§ï¼ˆ20ç‚¹ï¼‰""",
        'latest_execution_result': None,
        'current_workflow_execution': None,
        'workflow_execution_progress': {},
        'show_workflow_debug': False,
        'processing_mode': 'single',
        'current_workflow_steps': [],
        'temp_variables': ['input_1'],
        'temp_steps': [{}],
        # OpenAI specific UI states (if any, e.g. for 'instructions' field)
        # 'openai_instructions': "" 
    }
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

# Evaluator is now passed as an argument
def render_execution_tab(evaluator: Union[GeminiEvaluator, OpenAIEvaluator]):
    _initialize_session_state_exec_tab()

    header_col1, header_col2 = st.columns([3, 1])
    with header_col1:
        st.markdown("### ğŸš€ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
    with header_col2:
        current_branch = GitManager.get_current_branch()
        st.markdown(f"**ãƒ–ãƒ©ãƒ³ãƒ:** `{current_branch}`")

    st.markdown("#### å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ")
    mode_col1, mode_col2 = st.columns(2)
    with mode_col1:
        if st.button("ğŸ“ å˜ç™ºå‡¦ç†", use_container_width=True, help="1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦çµæœã‚’å–å¾—"):
            st.session_state.processing_mode = "single"
    with mode_col2:
        if st.button("ğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å‡¦ç†", use_container_width=True, help="è¤‡æ•°ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’é€£é–å®Ÿè¡Œã—ã¦æœ€çµ‚çµæœã‚’å–å¾—"):
            st.session_state.processing_mode = "workflow"

    if 'processing_mode' not in st.session_state:
        st.session_state.processing_mode = "single"
    st.markdown("---")

    if st.session_state.processing_mode == "single":
        _render_single_execution(evaluator) # Pass evaluator
    else:
        _render_workflow_execution(evaluator) # Pass evaluator

def _render_single_execution(evaluator: Union[GeminiEvaluator, OpenAIEvaluator]):
    st.markdown("### ğŸ“ å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
    # ... (rest of the single execution UI, form definition)
    with st.form("execution_form", clear_on_submit=False):
        memo_col1, memo_col2 = st.columns([4, 1])
        with memo_col1:
            execution_memo = st.text_input(
                "ğŸ“ å®Ÿè¡Œãƒ¡ãƒ¢", value=st.session_state.execution_memo,
                placeholder="å¤‰æ›´å†…å®¹ã‚„å®Ÿé¨“ç›®çš„...", key="memo_input_form"
            )
        with memo_col2:
            execution_mode_display = st.radio(
                "ãƒ¢ãƒ¼ãƒ‰", ["ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ", "å˜ä¸€"],
                index=0 if st.session_state.execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›" else 1,
                horizontal=True, key="mode_radio_form"
            )
        execution_mode_full = "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›" if execution_mode_display == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ" else "å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"

        prompt_template_val, user_input_data_val, single_prompt_val = _render_prompt_section_form(execution_mode_full)
        evaluation_criteria_val = _render_evaluation_section_form()
        
        # TODO: Add OpenAI 'instructions' field if desired
        # openai_instructions_val = ""
        # selected_model_cfg = get_model_config(st.session_state.selected_model)
        # if selected_model_cfg.get('api_provider') == 'openai':
        #    openai_instructions_val = st.text_area("OpenAI Instructions (Optional)", value=st.session_state.get('openai_instructions', ""), height=75, key="openai_instructions_form")

        submitted = st.form_submit_button("ğŸš€ å®Ÿè¡Œ & è‡ªå‹•è©•ä¾¡", type="primary", use_container_width=True)

    if submitted:
        st.session_state.execution_memo = execution_memo
        st.session_state.execution_mode = execution_mode_full
        st.session_state.prompt_template = prompt_template_val
        st.session_state.user_input_data = user_input_data_val
        st.session_state.single_prompt = single_prompt_val
        st.session_state.evaluation_criteria = evaluation_criteria_val
        # st.session_state.openai_instructions = openai_instructions_val # If field added

        placeholder_intermediate_resp = st.empty()
        placeholder_intermediate_metrics = st.empty()
        placeholder_final_eval_info = st.empty()

        _execute_prompt_and_evaluation_sequentially(
            evaluator, # Pass evaluator
            execution_memo, execution_mode_full,
            prompt_template_val, user_input_data_val, single_prompt_val, evaluation_criteria_val,
            placeholder_intermediate_resp, placeholder_intermediate_metrics, placeholder_final_eval_info
            # openai_instructions=openai_instructions_val # Pass if field added
        )

    if st.session_state.latest_execution_result:
        st.markdown("---")
        st.subheader("âœ… å®Ÿè¡Œãƒ»è©•ä¾¡å®Œäº†çµæœ")
        _display_latest_results()


def _render_workflow_execution(evaluator: Union[GeminiEvaluator, OpenAIEvaluator]):
    st.markdown("### ğŸ”„ å¤šæ®µéšãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ")
    st.caption("è¤‡æ•°ã®LLMå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †æ¬¡å®Ÿè¡Œã—ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ´»ç”¨ã§ãã¾ã™")

    workflow_tab1, workflow_tab2, workflow_tab3 = st.tabs([
        "ğŸ’¾ ä¿å­˜æ¸ˆã¿ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", "ğŸ†• æ–°è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆ", "ğŸ”§ é«˜åº¦ãªè¨­å®š"
    ])

    with workflow_tab1:
        _render_saved_workflow_execution(evaluator) # Pass evaluator
    with workflow_tab2:
        _render_workflow_builder(evaluator) # Pass evaluator
    with workflow_tab3:
        _render_advanced_workflow_settings(evaluator) # Pass evaluator


def _render_saved_workflow_execution(evaluator: Union[GeminiEvaluator, OpenAIEvaluator]):
    saved_workflows = WorkflowManager.get_saved_workflows()
    if not saved_workflows:
        st.info("ğŸ’¡ ä¿å­˜æ¸ˆã¿ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œæ–°è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆã€ã‚¿ãƒ–ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        # ... (help text)
        return

    # ... (workflow selection UI) ...
    workflow_col1, workflow_col2 = st.columns([3, 1])
    selected_id: Optional[str] = None
    with workflow_col1:
        workflow_options = {wid: f"{wdef.get('name', 'ç„¡åãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼')} ({len(wdef.get('steps', []))}ã‚¹ãƒ†ãƒƒãƒ—, {wdef.get('created_at', '')[:10] if wdef.get('created_at') else 'æ—¥ä»˜ä¸æ˜'})" for wid, wdef in saved_workflows.items()}
        if workflow_options:
            selected_id = st.selectbox("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é¸æŠ", options=list(workflow_options.keys()), format_func=lambda x: workflow_options[x], index=0, key="saved_wf_select")
    # ... (delete/duplicate buttons)
    with workflow_col2:
        if selected_id: # Only show buttons if a workflow is selected
            if st.button("ğŸ—‘ï¸ å‰Šé™¤", help="é¸æŠã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å‰Šé™¤", key=f"delete_wf_sidebar_{selected_id}"):
                if WorkflowManager.delete_workflow(selected_id):
                    st.success(f"âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€Œ{saved_workflows.get(selected_id, {}).get('name', selected_id)}ã€ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    st.rerun()
                else:
                    st.error("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

            if st.button("ğŸ“‹ è¤‡è£½", help="é¸æŠã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è¤‡è£½", key=f"duplicate_wf_sidebar_{selected_id}"):
                original_workflow = WorkflowManager.get_workflow(selected_id)
                if original_workflow:
                    new_name = f"{original_workflow.get('name', 'ç„¡åãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼')} (ã‚³ãƒ”ãƒ¼)"
                    new_id = WorkflowManager.duplicate_workflow(selected_id, new_name)
                    if new_id:
                        st.success(f"âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€Œ{new_name}ã€ã‚’ä½œæˆã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚")
                        st.rerun()
                    else:
                        st.error("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¤‡è£½ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    if selected_id:
        workflow_def = WorkflowManager.get_workflow(selected_id)
        if workflow_def:
            _render_workflow_info_panel(workflow_def)
            input_values = _render_workflow_input_section(workflow_def)
            execution_options = _render_execution_options()
            if st.button("ğŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ", type="primary", use_container_width=True, key=f"run_wf_main_{selected_id}"):
                _execute_workflow_with_progress(evaluator, workflow_def, input_values, execution_options) # Pass evaluator
        else:
             st.error(f"é¸æŠã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ID '{selected_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


def _render_workflow_builder(evaluator: Union[GeminiEvaluator, OpenAIEvaluator]): # Pass evaluator for test execution
    st.markdown("### ğŸ†• æ–°è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆ")
    # ... (rest of builder UI)
    with st.expander("ğŸ“ Step 1: åŸºæœ¬æƒ…å ±", expanded=True):
        basic_col1, basic_col2 = st.columns(2)
        with basic_col1:
            workflow_name = st.text_input("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å", value=st.session_state.get('wf_builder_name_cache', ""), placeholder="ä¾‹: æ–‡æ›¸åˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", key="wf_builder_name_input_main")
            st.session_state.wf_builder_name_cache = workflow_name
        with basic_col2:
            description = st.text_input("èª¬æ˜ï¼ˆä»»æ„ï¼‰", value=st.session_state.get('wf_builder_desc_cache', ""), placeholder="ä¾‹: æ–‡æ›¸ã‚’åˆ†æã—è¦ç´„ã¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ", key="wf_builder_desc_input_main")
            st.session_state.wf_builder_desc_cache = description

    # ... (global variables and steps setup) ...
    global_variables: List[str] = []
    input_values_for_test: Dict[str, str] = {}
    with st.expander("ğŸ“¥ Step 2: å…¥åŠ›å¤‰æ•°è¨­å®š", expanded=True):
        st.markdown("ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ä½¿ç”¨ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å…¥åŠ›å¤‰æ•°ã‚’å®šç¾©ã—ã¦ãã ã•ã„ï¼ˆä¾‹: `document_text`, `user_query`ï¼‰ã€‚")
        current_temp_vars = list(st.session_state.temp_variables)
        for i, var_name_in_session in enumerate(current_temp_vars):
            var_col1, var_col2, var_col3 = st.columns([2, 3, 1])
            with var_col1:
                new_var_name = st.text_input(f"å¤‰æ•°å {i+1}", value=var_name_in_session, key=f"var_name_builder_main_{i}", help="è‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿")
                if new_var_name != var_name_in_session:
                    if new_var_name.isidentifier(): st.session_state.temp_variables[i] = new_var_name
                    # ... (validation warnings)
                if new_var_name.isidentifier() and new_var_name not in global_variables: global_variables.append(new_var_name)
            with var_col2:
                if new_var_name and new_var_name.isidentifier():
                     input_values_for_test[new_var_name] = st.text_area(f"ã€Œ{new_var_name}ã€ã®ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿", value=st.session_state.get(f'var_test_builder_data_main_{new_var_name}', ""), key=f"var_test_builder_main_{i}", height=80)
                     st.session_state[f'var_test_builder_data_main_{new_var_name}'] = input_values_for_test[new_var_name]
            with var_col3:
                if len(st.session_state.temp_variables) > 1 and st.button("â–", key=f"remove_var_builder_main_{i}"):
                    st.session_state.temp_variables.pop(i)
                    st.rerun()
        if st.button("â• å¤‰æ•°ã‚’è¿½åŠ ", key="add_var_builder_main"):
            st.session_state.temp_variables.append(f"input_{len(st.session_state.temp_variables) + 1}")
            st.rerun()

    steps_config: List[Dict[str, Any]] = []
    with st.expander("ğŸ”§ Step 3: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—è¨­å®š", expanded=True):
        current_temp_steps = list(st.session_state.temp_steps)
        for i, step_data_in_session in enumerate(current_temp_steps):
            st.markdown(f"--- \n#### ğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ— {i+1}")
            step_col1, step_col2 = st.columns([3, 1])
            current_step_name = step_data_in_session.get('name', f"ã‚¹ãƒ†ãƒƒãƒ— {i+1}")
            available_vars_for_step = global_variables.copy()
            if i > 0: available_vars_for_step.extend([f"step_{j+1}_output" for j in range(i)])
            current_prompt_template = step_data_in_session.get('template', _get_default_prompt_template(i, available_vars_for_step))

            with step_col1:
                step_name_input = st.text_input("ã‚¹ãƒ†ãƒƒãƒ—å", value=current_step_name, key=f"step_name_builder_main_{i}")
            with step_col2:
                if len(st.session_state.temp_steps) > 1 and st.button("ğŸ—‘ï¸ ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤", key=f"remove_step_builder_main_{i}"):
                    st.session_state.temp_steps.pop(i)
                    st.rerun()
            _render_variable_help(available_vars_for_step)
            prompt_template_input = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ", value=current_prompt_template, key=f"step_prompt_builder_main_{i}", height=150)
            if st.checkbox(f"ã‚¹ãƒ†ãƒƒãƒ— {i+1} ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º", key=f"preview_builder_main_{i}"):
                _render_prompt_preview(prompt_template_input, input_values_for_test, i, steps_config) # Pass current steps_config
            
            current_step_config = {'name': step_name_input, 'prompt_template': prompt_template_input}
            steps_config.append(current_step_config)
            st.session_state.temp_steps[i] = current_step_config
        if st.button("â• ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ ", key="add_step_builder_main"):
            st.session_state.temp_steps.append({})
            st.rerun()

    st.markdown("### ğŸ¯ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    action_col1, action_col2, action_col3 = st.columns(3)
    with action_col1:
        if st.button("ğŸ’¾ ä¿å­˜", use_container_width=True, key="save_wf_builder_main"):
            if _validate_and_save_workflow(workflow_name, description, steps_config, global_variables):
                # Reset builder state
                st.session_state.wf_builder_name_cache = ""
                st.session_state.wf_builder_desc_cache = ""
                st.session_state.temp_variables = ['input_1']
                st.session_state.temp_steps = [{}]
                st.rerun()
    with action_col2:
        if st.button("ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ", use_container_width=True, key="test_wf_builder_main"):
            if workflow_name and steps_config:
                workflow_def_to_test = {'name': workflow_name, 'description': description, 'steps': steps_config, 'global_variables': global_variables}
                test_options = {'show_progress': True, 'debug_mode': True, 'cache_results': False, 'auto_retry': False}
                _execute_workflow_with_progress(evaluator, workflow_def_to_test, input_values_for_test, test_options) # Pass evaluator
            else:
                st.warning("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«ã¯ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã¨å°‘ãªãã¨ã‚‚1ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—å®šç¾©ãŒå¿…è¦ã§ã™ã€‚")
    with action_col3:
        if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ", use_container_width=True, key="reset_wf_builder_main"):
            # Reset builder state
            st.session_state.wf_builder_name_cache = ""
            st.session_state.wf_builder_desc_cache = ""
            st.session_state.temp_variables = ['input_1']
            st.session_state.temp_steps = [{}]
            st.rerun()


def _render_advanced_workflow_settings(evaluator: Union[GeminiEvaluator, OpenAIEvaluator]): # Pass evaluator
    st.markdown("### ğŸ”§ é«˜åº¦ãªè¨­å®š")
    # ... (cache management)
    st.markdown("#### ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†")
    if st.button("ğŸ—‘ï¸ ã‚¨ãƒ³ã‚¸ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", use_container_width=True, key="clear_engine_cache_advanced_main"):
        engine = WorkflowEngine(evaluator) # Initialize with current evaluator
        if hasattr(engine, 'clear_cache') and callable(engine.clear_cache):
            engine.clear_cache()
            st.success("âœ… WorkflowEngineã®å®Ÿè¡Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        else:
            st.info("ç¾åœ¨ã®WorkflowEngineã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã›ã‚“ã€‚")
            
    # ... (debug tools, export/import remains largely the same)
    st.markdown("---")
    st.markdown("#### ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«")
    debug_col1, debug_col2 = st.columns(2)
    with debug_col1:
        st.checkbox("è©³ç´°ãƒ­ã‚°å‡ºåŠ› (ã‚³ãƒ³ã‚½ãƒ¼ãƒ«)", key="debug_verbose_logging_adv_main", value=False)
        debug_show_substitution = st.checkbox("å¤‰æ•°ç½®æ›ã®è©³ç´°è¡¨ç¤º (UI)", key="debug_show_substitution_adv_main", value=st.session_state.get('show_workflow_debug', False))
        if debug_show_substitution != st.session_state.get('show_workflow_debug', False):
             st.session_state.show_workflow_debug = debug_show_substitution
             st.rerun()
    with debug_col2:
        debug_measure_time = st.checkbox("å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬ã®è©³ç´°è¡¨ç¤º (UI)", key="debug_measure_time_adv_main", value=st.session_state.get('show_workflow_debug', False))
        if debug_measure_time != st.session_state.get('show_workflow_debug', False):
             st.session_state.show_workflow_debug = debug_measure_time
             st.rerun()
        st.checkbox("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦– (ã‚³ãƒ³ã‚½ãƒ¼ãƒ«)", key="debug_monitor_memory_adv_main", value=False)
    
    st.markdown("---")
    st.markdown("#### ğŸ“¤ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©)")
    export_col1, export_col2 = st.columns(2)
    with export_col1:
        st.markdown("**ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**")
        saved_workflows_adv = WorkflowManager.get_saved_workflows()
        if saved_workflows_adv:
            workflow_names_adv = {wid: wdef.get('name', 'ç„¡åãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼') for wid, wdef in saved_workflows_adv.items()}
            selected_export_id_adv = st.selectbox("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", options=list(saved_workflows_adv.keys()), format_func=lambda x: workflow_names_adv.get(x,x), key="wf_export_select_adv_main")
            if selected_export_id_adv and st.button("ğŸ“¥ JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="wf_export_button_adv_main"):
                json_data_export_adv = WorkflowManager.export_workflow(selected_export_id_adv)
                if json_data_export_adv:
                    st.download_button("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=json_data_export_adv, file_name=f"{workflow_names_adv.get(selected_export_id_adv, selected_export_id_adv)}.json", mime="application/json", key="wf_export_download_adv_main")
        else:
            st.caption("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãªä¿å­˜æ¸ˆã¿ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    with export_col2:
        st.markdown("**ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**")
        uploaded_file_import_adv = st.file_uploader("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["json"], key="wf_import_uploader_adv_main")
        if uploaded_file_import_adv and st.button("ğŸ“¤ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", key="wf_import_button_adv_main"):
            try:
                json_data_str_import_adv = uploaded_file_import_adv.read().decode('utf-8')
                import_result_adv = WorkflowManager.import_workflow(json_data_str_import_adv)
                if import_result_adv.get('success'):
                    st.success(f"âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€Œ{import_result_adv.get('workflow_name', '')}ã€ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ (ID: {import_result_adv.get('workflow_id','')})")
                    st.rerun()
                else:
                    for err_item_adv in import_result_adv.get('errors', ['ä¸æ˜ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼']): st.error(f"âŒ {err_item_adv}")
            except Exception as e_import_adv:
                st.error(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e_import_adv)}")


def _execute_prompt_and_evaluation_sequentially(
    evaluator: Union[GeminiEvaluator, OpenAIEvaluator], # Added evaluator
    execution_memo: str, execution_mode: str,
    prompt_template_val: str, user_input_data_val: str, single_prompt_val: str, evaluation_criteria_val: str,
    placeholder_intermediate_resp: st.empty, placeholder_intermediate_metrics: st.empty, placeholder_final_eval_info: st.empty,
    openai_instructions: Optional[str] = None # Optional for OpenAI
):
    placeholder_intermediate_resp.empty()
    placeholder_intermediate_metrics.empty()
    placeholder_final_eval_info.empty()
    st.session_state.latest_execution_result = None

    validation_errors = _validate_inputs_direct(execution_memo, execution_mode, evaluation_criteria_val, prompt_template_val, user_input_data_val, single_prompt_val)
    if validation_errors:
        for err_msg in validation_errors: st.error(err_msg)
        return

    # Evaluator is already initialized and passed, API key and model config are handled there
    
    final_prompt_str: str = ""
    current_prompt_template_str: Optional[str] = None
    current_user_input_str: Optional[str] = None

    if execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›":
        final_prompt_str = prompt_template_val.replace("{user_input}", user_input_data_val)
        current_prompt_template_str = prompt_template_val
        current_user_input_str = user_input_data_val
    else:
        final_prompt_str = single_prompt_val

    # Prepare API call parameters
    api_call_params = {"prompt": final_prompt_str}
    # For OpenAI, if instructions are provided and evaluator is OpenAIEvaluator
    if isinstance(evaluator, OpenAIEvaluator) and openai_instructions:
        api_call_params["instructions"] = openai_instructions


    initial_exec_res: Optional[Dict[str, Any]] = None
    with st.spinner(f"ğŸ”„ {evaluator.get_model_info()}ã§ä¸€æ¬¡å®Ÿè¡Œä¸­..."): # Use evaluator.get_model_info()
        initial_exec_res = evaluator.execute_prompt(**api_call_params) # Use prepared params

    if not initial_exec_res or not initial_exec_res.get('success'):
        with placeholder_final_eval_info.container():
            error_msg_exec = initial_exec_res.get('error', 'ä¸æ˜ãªä¸€æ¬¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼') if initial_exec_res else 'ä¸€æ¬¡å®Ÿè¡ŒçµæœãŒã‚ã‚Šã¾ã›ã‚“'
            st.error(f"âŒ ä¸€æ¬¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {error_msg_exec}")
        return

    with placeholder_intermediate_resp.container():
        st.markdown("---"); st.subheader("ğŸ“ ä¸€æ¬¡å®Ÿè¡Œçµæœ (è©•ä¾¡å‰)")
        render_response_box(initial_exec_res['response_text'], f"ğŸ¤– LLMã®å›ç­” ({initial_exec_res.get('model_name', '')})")
    with placeholder_intermediate_metrics.container():
        st.markdown("##### ğŸ“Š ä¸€æ¬¡å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        cols_metrics_interim = st.columns(3)
        cols_metrics_interim[0].metric("å®Ÿè¡Œå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('input_tokens', 0):,}")
        cols_metrics_interim[1].metric("å®Ÿè¡Œå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('output_tokens', 0):,}")
        cols_metrics_interim[2].metric("å®Ÿè¡Œã‚³ã‚¹ãƒˆ(USD)", format_detailed_cost_display(initial_exec_res.get('cost_usd', 0.0)))
        st.info("è©•ä¾¡å‡¦ç†ã‚’è‡ªå‹•çš„ã«é–‹å§‹ã—ã¾ã™...")

    eval_res: Optional[Dict[str, Any]] = None
    with st.spinner("ğŸ“Š è©•ä¾¡å‡¦ç†ã‚’å®Ÿè¡Œä¸­..."):
        eval_res = evaluator.evaluate_response(
            original_prompt=final_prompt_str,
            llm_response_text=initial_exec_res['response_text'],
            evaluation_criteria=evaluation_criteria_val
        )

    if not eval_res or not eval_res.get('success'):
        with placeholder_final_eval_info.container():
            error_msg_eval = eval_res.get('error', 'ä¸æ˜ãªè©•ä¾¡å‡¦ç†ã‚¨ãƒ©ãƒ¼') if eval_res else 'è©•ä¾¡å‡¦ç†çµæœãŒã‚ã‚Šã¾ã›ã‚“'
            st.error(f"âŒ è©•ä¾¡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg_eval}")
            st.warning("ä¸€æ¬¡å®Ÿè¡Œã®çµæœã¯ä¸Šè¨˜ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ãŒã€è©•ä¾¡ã¯å¤±æ•—ã—ã¾ã—ãŸã€‚è¨˜éŒ²ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚")
        return

    placeholder_intermediate_resp.empty()
    placeholder_intermediate_metrics.empty()
    placeholder_final_eval_info.empty()

    exec_data_to_save: Dict[str, Any] = {
        'timestamp': datetime.datetime.now(),
        'execution_mode': execution_mode,
        'prompt_template': current_prompt_template_str,
        'user_input': current_user_input_str,
        'final_prompt': final_prompt_str,
        'criteria': evaluation_criteria_val,
        'response': initial_exec_res['response_text'],
        'evaluation': eval_res['response_text'],
        'execution_tokens': initial_exec_res.get('total_tokens', 0),
        'evaluation_tokens': eval_res.get('total_tokens', 0),
        'execution_cost': initial_exec_res.get('cost_usd', 0.0),
        'evaluation_cost': eval_res.get('cost_usd', 0.0),
        'total_cost': initial_exec_res.get('cost_usd', 0.0) + eval_res.get('cost_usd', 0.0),
        'model_name': initial_exec_res.get('model_name', 'N/A'),
        'model_id': initial_exec_res.get('model_id', 'N/A'),
        'api_provider': initial_exec_res.get('api_provider', 'gemini') # Store provider
    }
    exec_record = GitManager.create_commit(exec_data_to_save, execution_memo)
    GitManager.add_commit_to_history(exec_record)

    st.session_state.latest_execution_result = {
        'execution_result': initial_exec_res,
        'evaluation_result': eval_res,
        'execution_record': exec_record
    }
    st.success(f"âœ… å®Ÿè¡Œã¨è©•ä¾¡ãŒå®Œäº†ã—ã€è¨˜éŒ²ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ | ã‚³ãƒŸãƒƒãƒˆID: `{exec_record.get('commit_hash', 'N/A')}`")
    st.rerun()


def _execute_workflow_with_progress(
    evaluator: Union[GeminiEvaluator, OpenAIEvaluator], # Added evaluator
    workflow_def: Dict, input_values: Dict, options: Dict
):
    # API key check is implicitly handled by evaluator initialization in app.py
    # Model config is also handled by evaluator
    
    for var_name in workflow_def.get('global_variables', []):
        if not input_values.get(var_name, '').strip():
            st.error(f"âŒ å¿…é ˆå…¥åŠ›å¤‰æ•° '{var_name}' ã®å€¤ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

    engine = WorkflowEngine(evaluator) # Initialize with the correct evaluator

    st.markdown("### ğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œé€²æ—")
    overall_progress_container = st.container()
    steps_display_container = st.container()
    final_result_container = st.container()
    st.session_state.current_workflow_steps = []

    try:
        result_object = _execute_workflow_with_live_display(
            engine, workflow_def, input_values,
            overall_progress_container, steps_display_container, options
        )
        with final_result_container:
            st.markdown("---")
            st.markdown("### ğŸ¯ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†")
            _render_workflow_result(result_object, options.get('debug_mode', False))
    except Exception as e_exec_wf:
        st.error(f"âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e_exec_wf)}")


# Functions like _render_prompt_section_form, _render_evaluation_section_form,
# _display_latest_results, _render_workflow_info_panel, _render_workflow_input_section,
# _render_execution_options, _render_variable_help, _render_prompt_preview,
# _execute_workflow_with_live_display, _render_execution_progress, _render_workflow_result,
# _render_workflow_error, _validate_inputs_direct, _get_default_prompt_template,
# _validate_and_save_workflow remain largely the same but are called with the correct evaluator context.

# Helper functions (no change needed from original, just ensure they are present)
def _render_variable_help(available_vars: List[str]):
    if available_vars:
        st.markdown("**ğŸ’¡ åˆ©ç”¨å¯èƒ½ãªå¤‰æ•°:**")
        cols = st.columns(min(len(available_vars), 3) if len(available_vars) > 1 else 1) # Max 3 columns
        
        var_groups = {'ã‚°ãƒ­ãƒ¼ãƒãƒ«å…¥åŠ›': [], 'å‰ã®ã‚¹ãƒ†ãƒƒãƒ—çµæœ': []}
        for var in available_vars:
            if var.startswith('step_') and var.endswith('_output'):
                var_groups['å‰ã®ã‚¹ãƒ†ãƒƒãƒ—çµæœ'].append(var)
            else:
                var_groups['ã‚°ãƒ­ãƒ¼ãƒãƒ«å…¥åŠ›'].append(var)

        col_idx = 0
        for group_name, group_vars in var_groups.items():
            if group_vars:
                with cols[col_idx % len(cols)]:
                    st.markdown(f"*{group_name}:*")
                    for var in group_vars:
                        st.code(f"{{{var}}}")
                col_idx += 1

def _render_prompt_preview(template: str, input_values: Dict[str, str], current_step_index: int, previous_steps_config: List[Dict[str, Any]]):
    processor = VariableProcessor()
    context_for_preview = input_values.copy()
    for i, prev_step_conf in enumerate(previous_steps_config): # previous_steps_config is steps_config[:current_step_index]
        actual_prev_step_number = i + 1
        context_for_preview[f'step_{actual_prev_step_number}_output'] = f"[Step {actual_prev_step_number} ({prev_step_conf.get('name', '')}) ã®æ¨¡æ“¬å‡ºåŠ›]"
    try:
        preview_content = processor.substitute_variables(template, context_for_preview)
        st.markdown("**ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:**")
        display_preview = preview_content[:500] + ("..." if len(preview_content) > 500 else "")
        st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", value=display_preview, height=150, key=f"preview_text_area_{current_step_index}_{template[:10]}_main", disabled=True)
    except Exception as e:
        st.warning(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

def _render_prompt_section_form(execution_mode: str) -> Tuple[str, str, str]:
    st.markdown("### ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    prompt_template_val = st.session_state.get('prompt_template', "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{user_input}")
    user_input_data_val = st.session_state.get('user_input_data', "")
    single_prompt_val = st.session_state.get('single_prompt', "")
    if execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›":
        template_col1, template_col2 = st.columns(2)
        with template_col1:
            st.markdown("**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**"); prompt_template_val = st.text_area("", value=prompt_template_val, height=200, key="template_area_form_single_exec_main", label_visibility="collapsed")
        with template_col2:
            st.markdown("**ãƒ‡ãƒ¼ã‚¿**"); user_input_data_val = st.text_area("", value=user_input_data_val, height=200, key="data_area_form_single_exec_main", label_visibility="collapsed")
        if prompt_template_val and user_input_data_val and "{user_input}" in prompt_template_val and st.checkbox("ğŸ” æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºèª", key="preview_form_single_exec_main"):
            final_prompt_preview = prompt_template_val.replace("{user_input}", user_input_data_val)
            st.code(final_prompt_preview[:500] + "..." if len(final_prompt_preview)>500 else final_prompt_preview, language='text')
        elif prompt_template_val and "{user_input}" not in prompt_template_val and user_input_data_val.strip():
            st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã™ãŒã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ {user_input} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚")
    else:
        st.markdown("**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**"); single_prompt_val = st.text_area("", value=single_prompt_val, height=200, key="single_area_form_single_exec_main", label_visibility="collapsed")
    return prompt_template_val, user_input_data_val, single_prompt_val

def _render_evaluation_section_form() -> str:
    st.markdown("### ğŸ“‹ è©•ä¾¡åŸºæº–")
    return st.text_area("", value=st.session_state.get('evaluation_criteria', "1. æ­£ç¢ºæ€§ï¼ˆ30ç‚¹ï¼‰\n2. ç¶²ç¾…æ€§ï¼ˆ25ç‚¹ï¼‰..."), height=120, key="criteria_area_form_single_exec_main", label_visibility="collapsed")

def _display_latest_results():
    if not st.session_state.get('latest_execution_result'): return
    result_data = st.session_state.latest_execution_result
    initial_exec_res, eval_res = result_data.get('execution_result', {}), result_data.get('evaluation_result', {})
    result_col1, result_col2 = st.columns([2, 1])
    with result_col1:
        render_response_box(initial_exec_res.get('response_text', 'å¿œç­”ãªã—'), "ğŸ¤– LLMã®å›ç­”")
        render_evaluation_box(eval_res.get('response_text', 'è©•ä¾¡ãªã—'), "â­ è©•ä¾¡çµæœ")
    with result_col2:
        st.markdown("### ğŸ“Š å®Ÿè¡Œãƒ»è©•ä¾¡æƒ…å ±"); st.metric("ãƒ¢ãƒ‡ãƒ«å", initial_exec_res.get('model_name', 'N/A')); st.markdown("---")
        st.markdown("**å®Ÿè¡Œçµæœ**"); exec_cols = st.columns(2)
        exec_cols[0].metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('input_tokens', 0):,}")
        exec_cols[0].metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('total_tokens', 0):,}")
        exec_cols[1].metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{initial_exec_res.get('output_tokens', 0):,}")
        exec_cols[1].metric("ã‚³ã‚¹ãƒˆ", format_detailed_cost_display(initial_exec_res.get('cost_usd', 0.0)))
        st.markdown("---"); st.markdown("**è©•ä¾¡å‡¦ç†**"); eval_cols = st.columns(2)
        eval_cols[0].metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{eval_res.get('input_tokens', 0):,}")
        eval_cols[0].metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³", f"{eval_res.get('total_tokens', 0):,}")
        eval_cols[1].metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{eval_res.get('output_tokens', 0):,}")
        eval_cols[1].metric("ã‚³ã‚¹ãƒˆ", format_detailed_cost_display(eval_res.get('cost_usd', 0.0)))
        st.markdown("---"); st.metric("åˆè¨ˆã‚³ã‚¹ãƒˆ", format_detailed_cost_display(initial_exec_res.get('cost_usd', 0.0) + eval_res.get('cost_usd', 0.0)))

def _render_workflow_info_panel(workflow_def: Dict):
    st.markdown("#### ğŸ“Š ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è©³ç´°æƒ…å ±"); info_col1, info_col2, info_col3 = st.columns(3)
    created_date = (workflow_def.get('created_at', '')[:10] if workflow_def.get('created_at') else 'æ—¥ä»˜ä¸æ˜')
    info_col1.metric("ã‚¹ãƒ†ãƒƒãƒ—æ•°", len(workflow_def.get('steps', [])))
    info_col2.metric("å¿…è¦å¤‰æ•°æ•°", len(workflow_def.get('global_variables', [])))
    info_col3.metric("ä½œæˆæ—¥", created_date)
    if workflow_def.get('description'): st.markdown(f"**èª¬æ˜:** {workflow_def['description']}")
    st.markdown("**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹é€ :**")
    for i, step in enumerate(workflow_def.get('steps', [])):
        preview = step.get('prompt_template', '')[:100] + "..." if len(step.get('prompt_template', '')) > 100 else step.get('prompt_template', '')
        st.markdown(f"**Step {i+1}: {step.get('name', 'ç„¡åã‚¹ãƒ†ãƒƒãƒ—')}**\n```\n{preview}\n```")
        if i < len(workflow_def.get('steps', [])) - 1: st.markdown("â¬‡ï¸")
    st.markdown("---")

def _render_workflow_input_section(workflow_def: Dict) -> Dict[str, str]:
    input_values: Dict[str, str] = {}; global_vars = workflow_def.get('global_variables')
    if global_vars and isinstance(global_vars, list):
        st.markdown("### ğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿è¨­å®š")
        for var_name in global_vars:
            desc = _generate_variable_description(var_name)
            wf_id = workflow_def.get('id', workflow_def.get('name', 'unknown_workflow'))
            input_values[var_name] = st.text_area(f"**{var_name}**", help=desc, placeholder=f"{var_name}ã®å†…å®¹ã‚’å…¥åŠ›...", key=f"workflow_input_main_{wf_id}_{var_name}", height=120)
            if input_values[var_name]: st.caption(f"ğŸ“ {len(input_values[var_name]):,} æ–‡å­—")
    return input_values

def _generate_variable_description(var_name: str) -> str:
    descriptions = {'document': 'åˆ†æå¯¾è±¡ã®æ–‡æ›¸', 'data': 'å‡¦ç†ãƒ‡ãƒ¼ã‚¿', 'input': 'å…¥åŠ›æƒ…å ±', 'text': 'ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹', 'requirement': 'è¦ä»¶', 'context': 'èƒŒæ™¯æƒ…å ±'}
    for k, v in descriptions.items():
        if k in var_name.lower(): return f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ä½¿ç”¨ã™ã‚‹{v}"
    return f"å¤‰æ•° '{var_name}' ã®å€¤"

def _render_execution_options() -> Dict[str, Any]:
    with st.expander("âš™ï¸ å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
        col1, col2 = st.columns(2)
        show_progress = col1.checkbox("é€²æ—è¡¨ç¤º", value=True, key="wf_opt_show_progress_main")
        cache_results = col1.checkbox("çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨", value=True, key="wf_opt_cache_main")
        auto_retry = col2.checkbox("è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤", value=True, key="wf_opt_retry_main")
        debug_mode_ui = col2.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=st.session_state.get('show_workflow_debug', False), key="wf_opt_debug_main")
        if debug_mode_ui != st.session_state.get('show_workflow_debug', False):
            st.session_state.show_workflow_debug = debug_mode_ui; st.rerun()
        return {'show_progress': show_progress, 'cache_results': cache_results, 'auto_retry': auto_retry, 'debug_mode': debug_mode_ui}

def _execute_workflow_with_live_display(engine: WorkflowEngine, workflow_def: Dict, input_values: Dict, overall_progress_container: st.container, steps_display_container: st.container, options: Dict) -> WorkflowExecutionResult:
    exec_id = engine._generate_execution_id() if hasattr(engine, '_generate_execution_id') else f"temp-exec-id-{time.time()}"
    start_time_wf = datetime.datetime.now() # Renamed
    total_steps_wf = len(workflow_def.get('steps', [])) # Renamed
    exec_state = {'execution_id': exec_id, 'workflow_name': workflow_def.get('name', 'ç„¡å'), 'status': ExecutionStatus.RUNNING, 'current_step': 0, 'total_steps': total_steps_wf, 'start_time': start_time_wf, 'completed_step_result': None, 'error': None} # Renamed
    def update_overall_progress_local(): # Renamed
        if options.get('show_progress', True):
            with overall_progress_container: _render_execution_progress(exec_state, workflow_def)
    update_overall_progress_local()
    step_results_list_wf: List[StepResult] = [] # Renamed
    context_wf = input_values.copy() # Renamed
    for step_idx, step_cfg in enumerate(workflow_def.get('steps', [])): # Renamed
        current_step_num_wf = step_idx + 1 # Renamed
        step_start_time_inner = time.time() # Renamed
        step_name_wf = step_cfg.get('name', f'ã‚¹ãƒ†ãƒƒãƒ— {current_step_num_wf}') # Renamed
        exec_state.update({'current_step': current_step_num_wf, 'step_name': step_name_wf, 'completed_step_result': None})
        update_overall_progress_local()
        live_step_placeholder_ui: Optional[st.empty] = None # Renamed
        with steps_display_container: live_step_placeholder_ui = render_workflow_live_step(current_step_num_wf, step_name_wf, status="running")
        current_step_res_wf: StepResult # Renamed
        try:
            with st.spinner(f"Step {current_step_num_wf}: {step_name_wf} ã‚’å‡¦ç†ä¸­..."):
                current_step_res_wf = engine._execute_step_with_retry(step_cfg, context_wf, current_step_num_wf, exec_id, workflow_def.get('name', 'ç„¡å'), use_cache=options.get('cache_results', True), auto_retry=options.get('auto_retry', True))
        except Exception as e_step_exec_wf: # Renamed
            prompt_err_wf = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼" # Renamed
            try:
                if hasattr(engine, 'variable_processor') and isinstance(engine.variable_processor, VariableProcessor):
                    prompt_err_wf = engine.variable_processor.substitute_variables(step_cfg.get('prompt_template',''), context_wf)
            except: pass
            current_step_res_wf = StepResult(success=False, step_number=current_step_num_wf, step_name=step_name_wf, prompt=prompt_err_wf, response="", tokens=0, cost=0.0, execution_time=(time.time() - step_start_time_inner), error=f"ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œä¸­ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {str(e_step_exec_wf)}")
        if not hasattr(current_step_res_wf, 'execution_time') or current_step_res_wf.execution_time is None: current_step_res_wf.execution_time = time.time() - step_start_time_inner
        step_results_list_wf.append(current_step_res_wf)
        st.session_state.current_workflow_steps.append(current_step_res_wf)
        if live_step_placeholder_ui: live_step_placeholder_ui.empty()
        with steps_display_container: render_workflow_step_card(current_step_res_wf, current_step_num_wf, show_prompt=options.get('debug_mode', False), workflow_execution_id=exec_id)
        if not getattr(current_step_res_wf, 'success', False):
            err_detail_wf = getattr(current_step_res_wf, 'error', 'ä¸æ˜ãªã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼') # Renamed
            exec_state.update({'status': ExecutionStatus.FAILED, 'error': err_detail_wf})
            update_overall_progress_local()
            return engine._create_failure_result(exec_id, workflow_def.get('name', 'ç„¡å'), start_time_wf, err_detail_wf, step_results_list_wf)
        context_wf[f'step_{current_step_num_wf}_output'] = getattr(current_step_res_wf, 'response', "")
        exec_state['completed_step_result'] = current_step_res_wf
        if hasattr(current_step_res_wf, 'git_record') and current_step_res_wf.git_record: GitManager.add_commit_to_history(current_step_res_wf.git_record)
    exec_state.update({'status': ExecutionStatus.COMPLETED})
    update_overall_progress_local()
    return engine._create_success_result(exec_id, workflow_def.get('name', 'ç„¡å'), start_time_wf, step_results_list_wf)

def _render_execution_progress(state: Dict, workflow_def: Dict):
    status_wf, current_step_wf, total_steps_count_wf = state.get('status', ExecutionStatus.PENDING), state.get('current_step', 0), state.get('total_steps', len(workflow_def.get('steps', []))) # Renamed vars
    progress_val_wf = float(current_step_wf) / total_steps_count_wf if total_steps_count_wf > 0 else 0.0 # Renamed
    workflow_name_str_wf = state.get('workflow_name', workflow_def.get('name', 'ç„¡åãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼')) # Renamed
    if status_wf == ExecutionStatus.RUNNING:
        st.progress(progress_val_wf); step_name_str_wf = state.get('step_name', f'Step {current_step_wf}') # Renamed
        st.caption(f"å®Ÿè¡Œä¸­: {step_name_str_wf} ({current_step_wf}/{total_steps_count_wf} ã‚¹ãƒ†ãƒƒãƒ—) - {workflow_name_str_wf}")
    elif status_wf == ExecutionStatus.COMPLETED: st.progress(1.0); st.caption(f"ğŸ‰ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ '{workflow_name_str_wf}' å®Œäº†ï¼ ({total_steps_count_wf} ã‚¹ãƒ†ãƒƒãƒ—)")
    elif status_wf == ExecutionStatus.FAILED: st.progress(progress_val_wf); st.caption(f"âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ '{workflow_name_str_wf}' å¤±æ•—ã€‚({current_step_wf}/{total_steps_count_wf} ã§åœæ­¢) ã‚¨ãƒ©ãƒ¼: {state.get('error', 'ä¸æ˜')}")
    elif status_wf == ExecutionStatus.PENDING: st.caption(f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ '{workflow_name_str_wf}' æº–å‚™ä¸­...")

def _render_workflow_result(result: WorkflowExecutionResult, debug_mode: bool):
    render_workflow_result_tabs(result, debug_mode)
    if getattr(result, 'success', False):
        try:
            commit_data_wf = { # Renamed
                'timestamp': getattr(result, 'end_time', datetime.datetime.now()), 'execution_mode': 'ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ',
                'workflow_id': getattr(result, 'execution_id', 'N/A'), 'workflow_name': getattr(result, 'workflow_name', 'N/A'),
                'final_prompt': f"WF: {getattr(result, 'workflow_name', 'N/A')} ({len(getattr(result, 'steps',[]))}å®Œäº†)",
                'response': getattr(result, 'final_output', ""),
                'evaluation': f"WFæ­£å¸¸å®Œäº†: {len(getattr(result, 'steps',[]))}ã‚¹ãƒ†ãƒƒãƒ—, {getattr(result, 'duration_seconds', 0.0):.1f}ç§’",
                'execution_tokens': getattr(result, 'total_tokens', 0), 'evaluation_tokens': 0,
                'execution_cost': getattr(result, 'total_cost', 0.0), 'evaluation_cost': 0.0,
                'total_cost': getattr(result, 'total_cost', 0.0),
                'model_name': 'ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼', 'model_id': 'workflow_summary',
                'api_provider': 'workflow' # Generic provider for workflow summary
            }
            commit_msg_wf = f"WFå®Œäº†: {getattr(result, 'workflow_name', 'N/A')} (ID: {getattr(result, 'execution_id', 'N/A')})" # Renamed
            wf_git_record = GitManager.create_commit(commit_data_wf, commit_msg_wf) # Renamed
            GitManager.add_commit_to_history(wf_git_record)
            st.info(f"ğŸ“ WFå®Ÿè¡Œçµæœã‚’Gitå±¥æ­´ã«è¨˜éŒ² (Commit: `{wf_git_record.get('commit_hash', 'N/A')[:7]}`)")
        except Exception as e_git_wf: st.warning(f"âš ï¸ Gitå±¥æ­´ã¸ã®WFçµæœè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {str(e_git_wf)}") # Renamed
    else: _render_workflow_error(result)

def _render_workflow_error(result: WorkflowExecutionResult):
     workflow_name_err_wf = getattr(result, 'workflow_name', 'ç„¡åãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼') # Renamed
     st.error(f"âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œå¤±æ•—: {workflow_name_err_wf}")
     err_handler_wf = WorkflowErrorHandler() # Renamed
     err_msg_str_wf = str(getattr(result, 'error', "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")) # Renamed
     err_type_wf, desc_wf, suggestions_wf = err_handler_wf.categorize_error(err_msg_str_wf) # Renamed
     render_error_details(err_type_wf, desc_wf, suggestions_wf)
     steps_list_err_wf = getattr(result, 'steps', []) # Renamed
     if steps_list_err_wf:
         st.markdown("### ğŸ“‹ å®Œäº†æ¸ˆã¿ã‚¹ãƒ†ãƒƒãƒ— (ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå‰ã¾ã§)")
         for step_res_item_wf in steps_list_err_wf: # Renamed
             if getattr(step_res_item_wf, 'success', False): st.success(f"âœ… Step {getattr(step_res_item_wf, 'step_number', '?')}: {getattr(step_res_item_wf, 'step_name', 'ç„¡å')}")
             else: break

def _validate_inputs_direct(execution_memo: str, execution_mode: str, evaluation_criteria: str, prompt_template: str, user_input_data: str, single_prompt: str) -> List[str]:
    errors_val: List[str] = [] # Renamed
    if not execution_memo.strip(): errors_val.append("âŒ å®Ÿè¡Œãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    if execution_mode == "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›":
        if not prompt_template.strip(): errors_val.append("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        if "{user_input}" in prompt_template and not user_input_data.strip(): errors_val.append("âš ï¸ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ {user_input} ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        elif "{user_input}" not in prompt_template and user_input_data.strip(): errors_val.append("âš ï¸ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã« {user_input} ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    elif execution_mode == "å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ" and not single_prompt.strip(): errors_val.append("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif execution_mode not in ["ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ + ãƒ‡ãƒ¼ã‚¿å…¥åŠ›", "å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]: errors_val.append(f"âŒ ä¸æ˜ãªå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {execution_mode}")
    if not evaluation_criteria.strip(): errors_val.append("âŒ è©•ä¾¡åŸºæº–ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    return errors_val

def _get_default_prompt_template(step_index: int, available_vars: List[str]) -> str:
    if step_index == 0:
        first_global = next((v for v in available_vars if not v.startswith("step_")), None)
        return f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (å¤‰æ•°å: {first_global or 'input_data'}) ã‚’åˆ†æã—è¦ç‚¹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n{{{first_global or 'input_data'}}}" if first_global else "åˆæœŸãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚"
    else:
        prev_out_var = f"step_{step_index}_output" # step_index is 0-based, so step_1 output is for step_index=0 (next step is step_index=1)
        return f"å‰ã®ã‚¹ãƒ†ãƒƒãƒ— (Step {step_index}) ã®çµæœ:\n\n{{{prev_out_var}}}\n\nã“ã®çµæœã‚’è¸ã¾ãˆã€æ¬¡ã®æŒ‡ç¤ºã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚" if prev_out_var in available_vars else f"å‰ã®ã‚¹ãƒ†ãƒƒãƒ—çµæœã‚’åˆ©ç”¨ã—ã¦å‡¦ç†ã‚’ç¶™ç¶šã€‚(ã‚¨ãƒ©ãƒ¼: å¤‰æ•° {{{prev_out_var}}} ä¸æ˜)"

def _validate_and_save_workflow(name: str, description: str, steps: List[Dict[str,Any]], global_vars: List[str]) -> bool:
    if not name.strip(): st.error("âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åã‚’å…¥åŠ›ã€‚"); return False
    if not steps: st.error("âŒ å°‘ãªãã¨ã‚‚1ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ã€‚"); return False
    for i, step_item_val in enumerate(steps): # Renamed
        if not step_item_val.get('name','').strip(): st.error(f"âŒ ã‚¹ãƒ†ãƒƒãƒ— {i+1} ã®åå‰æœªå…¥åŠ›ã€‚"); return False
        if not step_item_val.get('prompt_template','').strip(): st.error(f"âŒ ã‚¹ãƒ†ãƒƒãƒ— {i+1} ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæœªå…¥åŠ›ã€‚"); return False
    wf_def_val: Dict[str, Any] = {'name': name, 'description': description, 'steps': steps, 'global_variables': global_vars} # Renamed
    validation_errors_wf = WorkflowManager.validate_workflow(wf_def_val) # Renamed
    if validation_errors_wf:
        for err_msg_val_wf in validation_errors_wf: st.error(f"âŒ {err_msg_val_wf}"); return False # Renamed
    wf_id_saved_val = WorkflowManager.save_workflow(wf_def_val) # Renamed
    if wf_id_saved_val: st.success(f"âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€Œ{name}ã€ä¿å­˜ (ID: {wf_id_saved_val})ã€‚"); return True
    else: st.error("âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¿å­˜å¤±æ•—ã€‚ãƒ­ã‚°ç¢ºèªã€‚"); return False